{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 3 - basic classifiers\n",
    "\n",
    "Math practice and coding application for main classifiers introduced in Chapter 3 of the Python machine learning book.\n",
    "\n",
    "### Huang Kai 3035086340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weighting\n",
    "\n",
    "Note that this assignment is more difficult than the previous ones, and thus has a higher weighting 3 and longer duration (3 weeks). Each one of the previous two assignments has a weighting 1.\n",
    "\n",
    "Specifically, the first 3 assignments contribute to your continuous assessment as follows:\n",
    "\n",
    "Assignment weights: $w_1 = 1, w_2 = 1, w_3 = 3$\n",
    "\n",
    "Assignment grades: $g_1, g_2, g_3$\n",
    "\n",
    "Weighted average: $\\frac{1}{\\sum_i w_i} \\times \\sum_i \\left(w_i \\times g_i \\right)$\n",
    "\n",
    "Future assignments will be added analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RBF kernel (20 points)\n",
    "\n",
    "Show that a Gaussian RBF kernel can be expressed as a dot product:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} \n",
    "= \\phi(\\mathbf{x})^T \\phi(\\mathbf{y})\n",
    "$$\n",
    "by spelling out the mapping function $\\phi$.\n",
    "\n",
    "For simplicity\n",
    "* you can assume both $\\mathbf{x}$ and $\\mathbf{y}$ are 2D vectors\n",
    "$\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    ", \\;\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "* we use a scalar unit variance here\n",
    "\n",
    "even though the proof can be extended for vectors $\\mathbf{x}$ $\\mathbf{y}$ and general covariance matrices.\n",
    "\n",
    "Hint: use Taylor series expansion of the exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Solution: RBF kernel\n",
    "$$\n",
    "\\begin{align}\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "&= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2}\n",
    "\\\\\n",
    "&= e^\\frac{-|(x_1-y_1, x_2-y_2)|^2}{2}\n",
    "\\\\\n",
    "&= e^\\frac{-(x_1-y_1)^2-(x_2-y_2)^2}{2}\n",
    "\\\\\n",
    "&= e^\\frac{-(x_1^2+x_2^2)-(y_1^2+y_2^2)+2(x_1y_1+x_2y_2)}{2}\n",
    "\\\\\n",
    "&= e^\\frac{-|\\mathbf{x}|^2}{2}e^\\frac{-|\\mathbf{y}|^2}{2}e^{\\mathbf{x}^T\\mathbf{y}}\n",
    "\\\\\n",
    "&= e^\\frac{-|\\mathbf{x}|^2}{2}e^\\frac{-|\\mathbf{y}|^2}{2} \\sum_{n=0}^{\\infty} \\frac {(\\mathbf{x}^T\\mathbf{y})^n}{n!}\n",
    "\\end{align}\n",
    "$$\n",
    "The last line above follows the Taylor Series Expansion of the exponential function.\n",
    "\n",
    "The $(\\mathbf{x}^T\\mathbf{y})^n$ term for each n is the kernel function of the polynomial kernel. We can use the Binomial Theorem to get the component entries:\n",
    "$$\n",
    "\\begin{align}\n",
    "(\\mathbf{x}^T\\mathbf{y})^n\n",
    "&= (x_1y_1+x_2y_2)^n\n",
    "\\\\\n",
    "&= \\sum_{i=0}^n \\binom{n}{i}(x_1y_1)^{n-i}(x_2y_2)^i\n",
    "\\\\\n",
    "&= \\sum_{i=0}^n \\binom{n}{i}x_1^{n-i}x_2^iy_1^{n-i}y_2^i\n",
    "\\end{align}\n",
    "$$\n",
    "Then, we can get the mapping function of the polynomial kernel of degree n. \n",
    "\n",
    "Let the mapping function be $\\varphi(x_1,x_2)$. It will have n entries in total.\n",
    "$$\n",
    "\\varphi(x_1,x_2) = [x_1^n,\\sqrt{\\binom{n}{1}}x_1^{n-1}x_2,\\sqrt{\\binom{n}{2}}x_1^{n-2}x_2^2,...,x_2^n]^T\n",
    "$$\n",
    "We can concatenate the entries of each degree polynomial kernel mapping function one after another to form the mapping function of the Gaussian RBF kernel that maps a 2D data to infinite dimensions. \n",
    "\n",
    "Finally, we can derive the mapping function of the Gaussian RBF kernel $\\phi$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi(\\mathbf{x})\n",
    "&=\\phi(x_1,x_2)\n",
    "\\\\\n",
    "&=e^{-\\frac{x_1^2+x_2^2}{2}}[1,\\sqrt{\\frac{1}{1!}}x_1,\\sqrt{\\frac{1}{1!}}x_2,\\sqrt{\\frac{1}{2!}}x_1^2,\\sqrt{\\frac{2}{2!}}x_1x_2, \\sqrt{\\frac{1}{2!}}x_2^2,\\sqrt{\\frac{1}{3!}}x_1^3,\\sqrt{\\frac{3}{3!}}x_1^2x_2,\\sqrt{\\frac{3}{3!}}x_2^1,\\sqrt{\\frac{1}{3!}}x_2^3,...\\sqrt{\\frac{\\binom{n}{i}}{n!}}x_1^ix_2^{n-i}...]^T\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel SVM complexity (10 points)\n",
    "\n",
    "How would the complexity (or number of parameters) of a kernel SVM change with the amount of training data, and why?\n",
    "Note that the answer may depend on the specific kernel used.\n",
    "Consider specifically the following types of kernels $K(\\mathbf{x}, \\mathbf{y})$.\n",
    "* linear:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\mathbf{x}^T \\mathbf{y}\n",
    "$$\n",
    "* polynomial with degree $q$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "(\\mathbf{x}^T\\mathbf{y} + 1)^q\n",
    "$$\n",
    "* RBF with distance function $D$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y} \\right) = e^{-\\frac{D\\left(\\mathbf{x}, \\mathbf{y} \\right)}{2s^2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Solution: Kernel SVM complexity\n",
    "Suppose the shape of the training data gives $(\\mathbf{n},\\mathbf{m})$, namely there are $\\mathbf{n}$ samples of training data and each data sample has $\\mathbf{m}$ features. We consider the following three kernels:\n",
    "#### Linear Kernel:\n",
    "The linear kernel projects to a feature space of dimension $\\mathbf{m}$, namely the original space. Thus the number of parameters is linear.\n",
    "\n",
    "To compute the kernel function $K\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\mathbf{x}^T \\mathbf{y}$ for each pair of data, complexity is $O(\\mathbf{m})$. Throughout the training, there are $O(\\mathbf{n}^2)$ pairs, the overall complexity of the Lagrangian multiplier process is $O(\\mathbf{m}\\mathbf{n}^2)$.\n",
    "\n",
    "#### Polynomial Kernel with degree q:\n",
    "The polynomial kernel projects to a new feature space of $\\mathbf{q}$ polynomial dimension with regard to $\\mathbf{m}$. Thus the number of parameters in polynomial.\n",
    "\n",
    "To compute the kernel function $K\\left(\\mathbf{x}, \\mathbf{y}\\right) = (\\mathbf{x}^T\\mathbf{y} + 1)^q$ for each pair of data, optimal complexity is approximately $O(\\mathbf{m}^{log{q}})$if we do the product in a recursive manner. Throughout the training, there are $O(\\mathbf{n}^2)$ pairs, the overall complexity of the Lagrangian multiplier process is $O(\\mathbf{m}^{log{q}}\\mathbf{n}^2)$.\n",
    "\n",
    "#### RBF Kernel with distance function D:\n",
    "The polynomial kernel projects to a new feature space of infinite dimensions. Thus the number of parameters tends to infinity.\n",
    "\n",
    "However, with the kernel trick, to compute the kernel function $K\\left(\\mathbf{x}, \\mathbf{y}\\right) = e^{-\\frac{D\\left(\\mathbf{x}, \\mathbf{y} \\right)}{2s^2}}$ for each pair of data, complexity is $O(\\mathbf{m})$if exponential calculation can be performed in constant time. Throughout the training, there are $O(\\mathbf{n}^2)$ pairs, the overall complexity of the Lagrangian multiplier process is $O(\\mathbf{m}\\mathbf{n}^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian density Bayes (30 points)\n",
    "\n",
    "$$\n",
    "p\\left(\\Theta | \\mathbf{X}\\right)\n",
    "= \n",
    "\\frac{p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)}{p\\left(\\mathbf{X}\\right)}\n",
    "$$\n",
    "\n",
    "Assume both the likelihood and prior have Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\Theta)\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "p(\\Theta)\n",
    "&=\n",
    "\\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derive $\\Theta$ from the dataset $\\mathbf{X}$ via the following methods:\n",
    "\n",
    "### ML (maximum likelihood) estimation \n",
    "$$\n",
    "\\Theta_{ML} = argmax_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "$$\n",
    "\n",
    "### MAP estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{MAP} \n",
    "&= \n",
    "argmax_{\\Theta} p(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&=\n",
    "argmax_{\\Theta} p(\\mathbf{X} | \\Theta) p(\\Theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Bayes estimation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{Bayes} \n",
    "&= \n",
    "E(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&= \n",
    "\\int \\Theta p(\\Theta | \\mathbf{X}) d\\Theta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Q3 Solution: Gaussian density Bayes\n",
    "\n",
    "### 1. ML (maximum likelihood) estimation\n",
    "We have the following:\n",
    "$$\n",
    "\\Theta_{ML} = argmax_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "\\\\\n",
    "p(\\mathbf{X} | \\Theta) = \\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "To find the $\\Theta$ that maximizes $p(\\mathbf{X} | \\Theta)$, by taking the negative logarithm on both sides, it is equivalent to find the $\\Theta$ that minimizes the following $f(\\mathbf{X}, \\Theta)$ given the dataset $\\mathbf{X}$:\n",
    "$$\n",
    "f(\\mathbf{X}, \\Theta) = \\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2\n",
    "$$\n",
    "We can take the derivative of $f(\\mathbf{X}, \\Theta)$ with regard to $\\Theta$ and set it to be 0 to find the $\\Theta_{ML}$ that minimizes it:\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{X}, \\Theta)}{\\partial \\Theta} = -2\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta) = 0\n",
    "$$\n",
    "By solving the above equation, we get:\n",
    "$$\n",
    "\\Theta_{ML} = \\mathbf{m} = \\frac {\\sum_{t=1}^N \\mathbf{x}^{(t)}} {N}\n",
    "$$\n",
    "\n",
    "### 2. MAP estimation\n",
    "We have the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{MAP} \n",
    "&= \n",
    "argmax_{\\Theta} p(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&=\n",
    "argmax_{\\Theta} p(\\mathbf{X} | \\Theta) p(\\Theta)\n",
    "\\end{align}\n",
    "\\\\\n",
    "p(\\mathbf{X} | \\Theta) p(\\Theta) = \\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "$$\n",
    "To find the $\\Theta$ that maximizes $p(\\mathbf{X} | \\Theta) p(\\Theta)$, by taking the negative logarithm on both sides, it is equivalent to find the $\\Theta$ that minimizes the following $g(\\mathbf{X}, \\Theta)$ given the dataset $\\mathbf{X}$:\n",
    "$$\n",
    "g(\\mathbf{X}, \\Theta) = \\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2} {2\\sigma^2} + \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\n",
    "$$\n",
    "We can take the derivative of $g(\\mathbf{X}, \\Theta)$ with regard to $\\Theta$ and set it to be 0 to find the $\\Theta_{MAP}$ that minimizes it:\n",
    "$$\n",
    "\\frac{\\partial g(\\mathbf{X}, \\Theta)}{\\partial \\Theta} = \\frac{1}{2\\sigma^2\\sigma_0^2} (2\\sigma_0^2 \\sum_{t=1}^N (\\Theta - \\mathbf{x}^{(t)}) + 2\\sigma^2 (\\Theta - \\mu_0)) = 0\n",
    "$$\n",
    "By solving the above equation, we get:\n",
    "$$\n",
    "\\Theta_{MAP} = \\frac{2\\sigma^2\\mu_0+2\\sigma_0^2\\sum_{t=1}^N \\mathbf{x}^{(t)}}{2\\sigma^2+2\\sigma_0^2N}\n",
    "\\\\\n",
    "= \\frac{\\sigma_0^2N}{\\sigma^2+\\sigma_0^2N} \\frac{\\sum_{t=1}^N \\mathbf{x}^{(t)}}{N}+ \\frac{\\sigma^2\\mu_0}{\\sigma^2+\\sigma_0^2N}\n",
    "\\\\\n",
    "= \\frac{N/\\sigma^2}{N/\\sigma^2+1/\\sigma_0^2}\\mathbf{m} + \\frac{1/\\sigma_0^2}{1/\\sigma_0^2+N/\\sigma^2}\n",
    "$$\n",
    "Thus, we reach the conclusion:\n",
    "$$\n",
    "\\Theta_{MAP} = \\frac{N/\\sigma^2}{N/\\sigma^2+1/\\sigma_0^2}\\mathbf{m} + \\frac{1/\\sigma_0^2}{1/\\sigma_0^2+N/\\sigma^2}\n",
    "$$\n",
    "where $\\mathbf{m}=\\Theta_{ML}$ in the maximum likelihood estimation.\n",
    "\n",
    "### 3. Bayes estimation\n",
    "Since we have $\\Theta_{Bayes} = E(\\Theta | \\mathbf{X})$, we need to find the expectation of $\\Theta$ given $\\mathbf{X}$. We first consider its density distribution function:\n",
    "$$\n",
    "p(\\Theta | \\mathbf{X}) = p(\\mathbf{X} | \\Theta) p(\\Theta)\n",
    "$$\n",
    "Both $p(\\mathbf{X} | \\Theta)$ and $p(\\Theta)$ have Gaussian density distribution, then $p(\\Theta | \\mathbf{X})$, namely the product of them, also has Gaussian density distribution.\n",
    "Since for the Gaussian Distribution, its expectation/mean has the largest value of density distribution, to find $E(\\Theta | \\mathbf{X})$ is equivalent to find $argmax_{\\Theta} p(\\mathbf{X} | \\Theta) p(\\Theta)$. That is to say, Bayes estimation and MAP estimation will lead to the same result.\n",
    "Therefore, we get:\n",
    "$$\n",
    "\\Theta_{Bayes} = \\Theta_{MAP} = \\frac{N/\\sigma^2}{N/\\sigma^2+1/\\sigma_0^2}\\mathbf{m} + \\frac{1/\\sigma_0^2}{1/\\sigma_0^2+N/\\sigma^2}\n",
    "$$\n",
    "where $\\mathbf{m}=\\Theta_{ML}$ in the maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hand-written digit classification (40 points)\n",
    "\n",
    "In the textbook sample code we applied different scikit-learn classifers for the Iris data set.\n",
    "\n",
    "In this exercise, we will apply the same set of classifiers over a different data set: hand-written digits.\n",
    "Please write down the code for different classifiers, choose their hyper-parameters, and compare their performance via the accuracy score as in the Iris dataset.\n",
    "Which classifier(s) perform(s) the best and worst, and why?\n",
    "\n",
    "The classifiers include:\n",
    "* perceptron\n",
    "* logistic regression\n",
    "* SVM\n",
    "* decision tree\n",
    "* random forest\n",
    "* KNN\n",
    "* naive Bayes\n",
    "\n",
    "The dataset is available as part of scikit learn, as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data # training data\n",
    "y = digits.target # training label\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1851ef1a9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFuCAYAAADETwDwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1851d211f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "\n",
    "index = 12\n",
    "pl.gray()\n",
    "pl.matshow(digits.images[index])\n",
    "pl.title('digit ' + str(digits.target[index]))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1851fb31940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for index in range(num_rows*num_cols):\n",
    "    img = digits.images[index]\n",
    "    label = digits.target[index]\n",
    "    ax[index].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[index].set_title('digit ' + str(label))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Date Preprocessing\n",
    "Hint: How you divide training and test data set? And apply other techinques we have learned if needed.\n",
    "You could take a look at the Iris data set case in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1257, test: 540\n"
     ]
    }
   ],
   "source": [
    "#Your code comes here\n",
    "#I use sklearn version 0.18 so I just import from sklearn.model_selection instead of sklearn.cross_validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# splitting data into 70% training and 30% test data: \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "num_training = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "print('training: ' + str(num_training) + ', test: ' + str(num_test))\n",
    "\n",
    "#Data scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #1 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 39 out of 540\n",
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "#Training\n",
    "ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "#Testing\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 35 out of 540\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Training\n",
    "lr = LogisticRegression(C=1000.0, random_state=0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "#Testing\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #3 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples using Linear Kernel: 16 out of 540\n",
      "Accuracy using Linear Kernel: 0.97\n",
      "Misclassified samples using Polynomial Kernel: 6 out of 540\n",
      "Accuracy using Polynomial Kernel: 0.99\n",
      "Misclassified samples using RBF Kernel: 28 out of 540\n",
      "Accuracy using RBF Kernel: 0.95\n",
      "Misclassified samples using Sigmoid Kernel: 19 out of 540\n",
      "Accuracy using Sigmoid Kernel: 0.96\n",
      "\n",
      "If we use data without normalization, we can get the following results:\n",
      "\n",
      "Misclassified samples using Linear Kernel: 14 out of 540\n",
      "Accuracy using Linear Kernel: 0.97\n",
      "Misclassified samples using Polynomial Kernel: 11 out of 540\n",
      "Accuracy using Polynomial Kernel: 0.98\n",
      "Misclassified samples using RBF Kernel: 89 out of 540\n",
      "Accuracy using RBF Kernel: 0.84\n",
      "Misclassified samples using Sigmoid Kernel: 179 out of 540\n",
      "Accuracy using Sigmoid Kernel: 0.67\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Linear Kernel\n",
    "#Training\n",
    "svm = SVC(kernel='linear', random_state=0, gamma=0.2, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "#Testing\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Misclassified samples using Linear Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy using Linear Kernel: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Polynomial Kernel\n",
    "svm = SVC(kernel='poly', random_state=0, gamma=0.2, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Misclassified samples using Polynomial Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy using Polynomial Kernel: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#RBF Kernel\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=0.1, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Misclassified samples using RBF Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy using RBF Kernel: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Sigmoid Kernel\n",
    "svm = SVC(kernel='sigmoid', random_state=0, gamma=0.01, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Misclassified samples using Sigmoid Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy using Sigmoid Kernel: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Use data without normalization\n",
    "print('\\nIf we use data without normalization, we can get the following results:\\n')\n",
    "#Linear Kernel\n",
    "svm = SVC(kernel='linear', random_state=0, gamma=0.2, C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('Misclassified samples using Linear Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy using Linear Kernel: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Polynomial Kernel\n",
    "svm = SVC(kernel='poly', random_state=0, gamma=0.2, C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('Misclassified samples using Polynomial Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy using Polynomial Kernel: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#RBF Kernel\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=0.01, C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('Misclassified samples using RBF Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy using RBF Kernel: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Sigmoid Kernel\n",
    "svm = SVC(kernel='sigmoid', random_state=0, gamma=0.001, C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('Misclassified samples using Sigmoid Kernel: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy using Sigmoid Kernel: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 71 out of 540\n",
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Training. For Decision Tree, just use original data without normalization.\n",
    "#Here to increase accuracy, I don't restrict the maximum depth to 3.\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "#Testing\n",
    "y_pred = tree.predict(X_test)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifer #5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 19 out of 540\n",
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Training. For Random Forest, just use original data without normalization.\n",
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=20, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "#Testing\n",
    "y_pred = forest.predict(X_test)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #6 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 29 out of 540\n",
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Training\n",
    "knn = KNeighborsClassifier(n_neighbors=25, p=2, metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "#Testing\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #7 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 125 out of 540\n",
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Training\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_std, y_train)\n",
    "#Testing\n",
    "y_pred = gnb.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Observations\n",
    "For each classifier, I adjusted the hyper-parameter to get a relatively better accuracy.\n",
    "\n",
    "After comparing the accuracies printed above, Polynomial Kernel SVM has the best accuracy. This is probably because for the polynomial kernel function, data are mapped to a higher outer space that appropriately fits the data to be separated without causing overfit.\n",
    "Besides, Random Forest and KNN have good accuracy as well.\n",
    "Naive Bayes has the worst accuracy possibly due to the reason that it assumes that the features are independent for the likelihood. However, in this case, the pixels of a digit are actually dependent of each other and thus this causes potential misclassifications.\n",
    "\n",
    "I also observed that the performance of different SVMs varies a lot and the choice of gamma value influences the accuracy greatly. In addition, standardized data will result in a higher accuracy for SVM."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
